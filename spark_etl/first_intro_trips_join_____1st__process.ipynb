{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "import os, glob\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the spark session instance \n",
    "spark=SparkSession.builder.appName(\"nyc\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 Apr 28\n"
     ]
    }
   ],
   "source": [
    "# getting current date, month and day\n",
    "today = date.today()\n",
    "year = (today.strftime(\"%Y\"))\n",
    "month = (today.strftime(\"%B\"))[:3]\n",
    "day = (today.strftime(\"%d\"))\n",
    "print(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2021\n",
    "# month =\n",
    "# day = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### This is main PATH for nyc text data #####################\n",
    "FETCH_FOLDER = \"/opt/notebooks/nyc/raw_zone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### This cell is for joining the PATH with RAW_ZONE #####################\n",
    "def trip_path_join(new_path):\n",
    "    return os.path.join(FETCH_FOLDER+new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Reading and Converting into DataFrame #####################\n",
    "def convert_parquet_to_dataframe(trips_complete_path):\n",
    "    try:\n",
    "        df=spark.read.parquet(trips_complete_path)\n",
    "    except FileNotFoundError as e:\n",
    "        df = None\n",
    "        print(\"no such file\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/notebooks/nyc/raw_zone/master/ratecodes/2022_04_28_1651148941115_0.parquet', '/opt/notebooks/nyc/raw_zone/master/ratecodes/2022_04_28_1651148981265_0.parquet']\n"
     ]
    }
   ],
   "source": [
    "##########        Testing      #####################\n",
    "\n",
    "\n",
    "\n",
    "print(glob.glob(trip_path_join(\"/master/ratecodes/*.parquet\")))\n",
    "# trips_parquet_path = os.path.join(trip_path_join(\"/trips/2021/Nov/26/*.parquet\"))\n",
    "# df = convert_parquet_to_dataframe(trips_parquet_path)\n",
    "# print(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With  Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for path ( ratecode )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ratecode_val = False\n",
    "if (trip_path_join(\"/master/ratecodes/\")):\n",
    "    if len(glob.glob(trip_path_join(\"/master/ratecodes/*\"))):\n",
    "        ratecode_val = True\n",
    "        ratecode_df = convert_parquet_to_dataframe(trip_path_join(\"/master/ratecodes/*.parquet\"))\n",
    "\n",
    "        \n",
    "print(ratecode_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for path ( payments )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "payment_val = False\n",
    "if (trip_path_join(\"/master/payments/\")):\n",
    "    if len(glob.glob(trip_path_join(\"/master/payments/*\"))):\n",
    "        payment_val = True\n",
    "        payment_df = convert_parquet_to_dataframe(trip_path_join(\"/master/payments/*.parquet\"))\n",
    "\n",
    "        \n",
    "print(payment_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for path ( coordinates )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "coordinate_val = False\n",
    "if (trip_path_join(\"/master/coordinates/\")):\n",
    "    if len(glob.glob(trip_path_join(\"/master/coordinates/*\"))):\n",
    "        coordinate_val = True\n",
    "        coordinate_df = convert_parquet_to_dataframe(trip_path_join(\"/master/coordinates/*.parquet\"))\n",
    "\n",
    "        \n",
    "print(coordinate_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for path ( zone )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "zone_val = False\n",
    "if (trip_path_join(\"/master/zone/\")):\n",
    "    if len(glob.glob(trip_path_join(\"/master/zone/*\"))):\n",
    "        zone_val = True\n",
    "        zone_df = convert_parquet_to_dataframe(trip_path_join(\"/master/zone/*.parquet\"))\n",
    "\n",
    "        \n",
    "print(zone_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(trip_path_join(\"/trips/{0}/{1}/{2}/*\".format(2021,'Nov',29)))\n",
    "# print(trip_path_join(\"/trips/{0}/{1}/{2}/*\".format(2021,'Nov',29)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for path ( Trips )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n",
      "Apr\n",
      "+--------------------+---------+-------+\n",
      "|         vendor_name|vendor_id|trip_id|\n",
      "+--------------------+---------+-------+\n",
      "|       VeriFone Inc.|      2.0|  299.0|\n",
      "|Creative Mobile T...|      1.0|  305.0|\n",
      "|       VeriFone Inc.|      2.0|  496.0|\n",
      "|Creative Mobile T...|      1.0|  558.0|\n",
      "|Creative Mobile T...|      1.0|  596.0|\n",
      "|       VeriFone Inc.|      2.0|  692.0|\n",
      "|       VeriFone Inc.|      2.0|  769.0|\n",
      "|Creative Mobile T...|      1.0|  934.0|\n",
      "|       VeriFone Inc.|      2.0| 1051.0|\n",
      "|Creative Mobile T...|      1.0| 1761.0|\n",
      "|       VeriFone Inc.|      2.0| 2734.0|\n",
      "|       VeriFone Inc.|      2.0| 2815.0|\n",
      "|       VeriFone Inc.|      2.0| 2862.0|\n",
      "|       VeriFone Inc.|      2.0| 3597.0|\n",
      "|Creative Mobile T...|      1.0| 3901.0|\n",
      "|       VeriFone Inc.|      2.0| 3980.0|\n",
      "|       VeriFone Inc.|      2.0| 4066.0|\n",
      "|Creative Mobile T...|      1.0| 4142.0|\n",
      "|       VeriFone Inc.|      2.0| 4800.0|\n",
      "|       VeriFone Inc.|      2.0| 5360.0|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame[trip_id: double, pickup_lattitude: double, pickup_longitude: double, pickup_country_name: string, pickup_country_code: string, pickup_region: string, pickup_region_code: string, drop_lattitude: double, drop_longitude: double, drop_country_name: string, drop_country_code: string, drop_region: string, drop_region_code: string, payment_type_id: double, rate_code_id: double, vendor_id: double, tpep_pickup_datetime: string, passenger_count: double, trip_distance: double, pickup_borough: string, pickup_zone: string, pickup_service_zone: string, store_and_fwd_flag: boolean, pu_location_id: double, congestion_surcharge: double, total_amount: double, improvement_surcharge: double, tolls_amount: double, tip_amount: double, mta_tax: double, extra: double, fare_amount: double, do_location_id: double, tpep_dropoff_datetime: string, drop_zone: string, drop_borough: string, drop_service_zone: string, _airbyte_ab_id: string, _airbyte_emitted_at: timestamp, type: string, _airbyte_additional_properties: map<string,string>, _airbyte_ab_id: string, _airbyte_emitted_at: timestamp, mode: string, _airbyte_additional_properties: map<string,string>, vendor_name: string]\n",
      "1211031\n",
      "+--------------------+---------------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+--------------------+---------------------+\n",
      "|2020-01-01T00:49:...| 2020-01-01T00:51:...|\n",
      "|2020-01-01T00:16:...| 2020-01-01T00:28:...|\n",
      "|2020-01-01T00:07:...| 2020-01-01T00:10:...|\n",
      "|2020-01-01T00:21:...| 2020-01-01T00:28:...|\n",
      "|2020-01-01T00:16:...| 2020-01-01T00:20:...|\n",
      "|2020-01-01T00:43:...| 2020-01-01T00:50:...|\n",
      "|2020-01-01T00:34:...| 2020-01-01T00:45:...|\n",
      "|2020-01-01T00:12:...| 2020-01-01T00:22:...|\n",
      "|2020-01-01T00:55:...| 2020-01-01T01:19:...|\n",
      "|2020-01-01T00:47:...| 2020-01-01T01:05:...|\n",
      "|2020-01-01T00:48:...| 2020-01-01T01:03:...|\n",
      "|2020-01-01T00:54:...| 2020-01-01T01:24:...|\n",
      "|2020-01-01T00:09:...| 2020-01-01T00:11:...|\n",
      "|2020-01-01T00:54:...| 2020-01-01T00:56:...|\n",
      "|2020-01-01T00:13:...| 2020-01-01T00:25:...|\n",
      "|2020-01-01T00:38:...| 2020-01-01T00:46:...|\n",
      "|2020-01-01T00:37:...| 2020-01-01T00:54:...|\n",
      "|2020-01-01T00:14:...| 2020-01-01T00:33:...|\n",
      "|2020-01-01T00:19:...| 2020-01-01T00:33:...|\n",
      "|2020-01-01T00:26:...| 2020-01-01T00:52:...|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+--------------------+-------+\n",
      "|vendor_id|         vendor_name|trip_id|\n",
      "+---------+--------------------+-------+\n",
      "|      2.0|       VeriFone Inc.|  299.0|\n",
      "|      1.0|Creative Mobile T...|  305.0|\n",
      "|      2.0|       VeriFone Inc.|  496.0|\n",
      "|      1.0|Creative Mobile T...|  558.0|\n",
      "|      1.0|Creative Mobile T...|  596.0|\n",
      "|      2.0|       VeriFone Inc.|  692.0|\n",
      "|      2.0|       VeriFone Inc.|  769.0|\n",
      "|      1.0|Creative Mobile T...|  934.0|\n",
      "|      2.0|       VeriFone Inc.| 1051.0|\n",
      "|      1.0|Creative Mobile T...| 1761.0|\n",
      "|      2.0|       VeriFone Inc.| 2734.0|\n",
      "|      2.0|       VeriFone Inc.| 2815.0|\n",
      "|      2.0|       VeriFone Inc.| 2862.0|\n",
      "|      2.0|       VeriFone Inc.| 3597.0|\n",
      "|      1.0|Creative Mobile T...| 3901.0|\n",
      "|      2.0|       VeriFone Inc.| 3980.0|\n",
      "|      2.0|       VeriFone Inc.| 4066.0|\n",
      "|      1.0|Creative Mobile T...| 4142.0|\n",
      "|      2.0|       VeriFone Inc.| 4800.0|\n",
      "|      2.0|       VeriFone Inc.| 5360.0|\n",
      "+---------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "trip_val = False\n",
    "if (os.path.exists(trip_path_join(\"/trips/{0}\".format(year)))):\n",
    "    print(year)\n",
    "    if (os.path.exists(trip_path_join(\"/trips/{0}/{1}\".format(year,month)))):\n",
    "        print(month)\n",
    "        if (trip_path_join(\"/trips/{0}/{1}/{2}\".format(year,month,day))):\n",
    "            if len(glob.glob(trip_path_join(\"/trips/{0}/{1}/{2}/*\".format(year,month,day)))):\n",
    "                trip_val = True\n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            Reading Trip Parquet DataSet on Current Date\n",
    "                #################################################################\n",
    "                yellow_taxi_trip_df = convert_parquet_to_dataframe(os.path.join(trip_path_join(\"/trips/{0}/{1}/{2}/*.parquet\".format(year,month,day))))\n",
    "                \n",
    "            \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe contains necessary information related to pick up joined with trip_data\n",
    "                pickup_zone = yellow_taxi_trip_df.join(zone_df, yellow_taxi_trip_df.pu_location_id == zone_df.location_id,\n",
    "                                       \"inner\").select(\"trip_id\", \"vendor_id\",\n",
    "                                                       \"tpep_pickup_datetime\", \"passenger_count\", \"trip_distance\",\n",
    "                                                       \"rate_code_id\", col(\"borough\").alias(\"pickup_borough\"),\n",
    "                                                       col(\"zone\").alias(\"pickup_zone\"),\n",
    "                                                       col(\"service_zone\").alias(\"pickup_service_zone\"),\n",
    "                                                       \"store_and_fwd_flag\", \"pu_location_id\")\n",
    "\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe contains the drop related data joined with trip data \n",
    "                drop_zone_df = yellow_taxi_trip_df.join(\n",
    "                    zone_df, yellow_taxi_trip_df.do_location_id == zone_df.location_id,\n",
    "                    \"inner\").select(\"trip_id\", \"congestion_surcharge\", \"total_amount\",\n",
    "                                    \"improvement_surcharge\", \"tolls_amount\",\n",
    "                                    \"tip_amount\", \"mta_tax\", \"extra\", \"fare_amount\",\n",
    "                                    \"payment_type_id\", \"do_location_id\", \"tpep_dropoff_datetime\",\n",
    "                                    col(\"zone\").alias(\"drop_zone\"),\n",
    "                                    col(\"borough\").alias(\"drop_borough\"),\n",
    "                                    col(\"service_zone\").alias(\"drop_service_zone\"))\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe containes the combine information of pick up and delivery\n",
    "                pickup_drop_df = pickup_zone.join(drop_zone_df, [\"trip_id\"])\n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                ratecode_without_dates=ratecode_df.drop(\"created_at\",\"updated_at\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # dropping created_at and updated_at columns from payment dataframe to avoid column redundancy in the final dataframe\n",
    "                payment_type_without_dates=payment_df.drop(\"created_at\",\"updated_at\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe contains combine information of rates,payment type \n",
    "                combined_df = pickup_drop_df.join(ratecode_without_dates, [\"rate_code_id\"]).join(payment_type_without_dates, [\"payment_type_id\"])\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe contains pickup cordinates\n",
    "                cordinates_pickup = coordinate_df.select(col(\"coordinate_id\").alias(\"pickup_coordinate_id\"),\n",
    "                      col(\"latitude\").alias(\"pickup_lattitude\"),\n",
    "                      col(\"longitude\").alias(\"pickup_longitude\"),\n",
    "                      col(\"country_name\").alias(\"pickup_country_name\"),\n",
    "                      col(\"country_code\").alias(\"pickup_country_code\"),\n",
    "                      col(\"region\").alias(\"pickup_region\"),\n",
    "                      col(\"region_code\").alias(\"pickup_region_code\"), \"location_id\")\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # this dataframe contains drop cordinates information\n",
    "                cordinates_dropoff = coordinate_df.select(col(\"coordinate_id\").alias(\"drop_coordinate_id\"),\n",
    "                       col(\"latitude\").alias(\"drop_lattitude\"),\n",
    "                       col(\"longitude\").alias(\"drop_longitude\"),\n",
    "                       col(\"country_name\").alias(\"drop_country_name\"),\n",
    "                       col(\"country_code\").alias(\"drop_country_code\"),\n",
    "                       col(\"region\").alias(\"drop_region\"),\n",
    "                       col(\"region_code\").alias(\"drop_region_code\"), \"location_id\")\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                yellow_with_Only_pickup_cordinates = yellow_taxi_trip_df.join(cordinates_pickup,\n",
    "                    cordinates_pickup.location_id == yellow_taxi_trip_df.pu_location_id).select(\n",
    "                        \"trip_id\", \"pickup_lattitude\",\n",
    "                        \"pickup_longitude\",\n",
    "                        \"pickup_country_name\",\n",
    "                        \"pickup_country_code\",\n",
    "                        \"pickup_region\",\n",
    "                        \"pickup_region_code\")\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                yellow_with_Only_dropoff_cordinates = yellow_taxi_trip_df.join(cordinates_dropoff,\n",
    "                    cordinates_dropoff.location_id == yellow_taxi_trip_df.do_location_id).select(\n",
    "                    \"trip_id\", \"drop_lattitude\",\n",
    "                    \"drop_longitude\",\n",
    "                    \"drop_country_name\",\n",
    "                    \"drop_country_code\",\n",
    "                    \"drop_region\",\n",
    "                    \"drop_region_code\")\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                processed_df = yellow_with_Only_pickup_cordinates.join(yellow_with_Only_dropoff_cordinates, [\"trip_id\"]).join(combined_df, [\"trip_id\"])\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                vendor_map={1:\"Creative Mobile Technologies, LLC\",2:\"VeriFone Inc.\"}\n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # adding vendor name associated with corresponding vendor_id and adding column for the vendor name\n",
    "                final_processed_df = processed_df.withColumn(\"vendor_name\",when(processed_df.vendor_id==1,vendor_map.get(1)).when(processed_df.vendor_id==2,vendor_map.get(2)).otherwise(\"no such vendor\"))\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                final_processed_df.select(\"vendor_name\",\"vendor_id\",\"trip_id\").show()\n",
    "                \n",
    "                \n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                print(final_processed_df)\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # changing the sequence of the columns to make it more readable\n",
    "                final_processed_df=final_processed_df.select(col(\"trip_id\"),col(\"vendor_id\"),col(\"vendor_name\"),col(\"pu_location_id\"),col(\"pickup_zone\"),\n",
    "                          col(\"pickup_service_zone\"),col(\"pickup_region_code\"),col(\"pickup_region\"),col(\"pickup_country_code\"),\n",
    "                          col(\"pickup_country_name\"),col(\"pickup_lattitude\"),col(\"pickup_longitude\"),col(\"pickup_borough\"),\n",
    "                          col(\"tpep_pickup_datetime\"),col(\"do_location_id\"),col(\"drop_region_code\"),col(\"drop_region\"),col(\"drop_country_code\"),\n",
    "                          col(\"drop_country_name\"),col(\"drop_zone\"),col(\"drop_service_zone\"),col(\"drop_lattitude\"),col(\"drop_longitude\"),\n",
    "                          col(\"drop_borough\"),col(\"tpep_dropoff_datetime\"),col(\"rate_code_id\"),col(\"type\"),col(\"passenger_count\"),col(\"trip_distance\"),col(\"store_and_fwd_flag\"),\n",
    "                          col(\"payment_type_id\"),col(\"mode\"),col(\"congestion_surcharge\"),col(\"improvement_surcharge\"),col(\"tolls_amount\"),\n",
    "                          col(\"tip_amount\"),col(\"mta_tax\"),col(\"fare_amount\"),col(\"extra\"),col(\"total_amount\")\n",
    "                         )\n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                print(final_processed_df.count())\n",
    "                final_processed_df.select('tpep_pickup_datetime','tpep_dropoff_datetime').show()\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                final_processed_df.select(\"vendor_id\",\"vendor_name\",\"trip_id\").show()\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                len(final_processed_df.columns)\n",
    "                \n",
    "                #################################################################\n",
    "                #            \n",
    "                #################################################################\n",
    "                # writing single processed parquet file\n",
    "                final_processed_df.repartition(1).write.format(\"parquet\").mode(\"append\").save(f\"/opt/notebooks/nyc/processed_zone/trips/year={year}/month={month}/day={day}/first.parquet\")\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "print(trip_val)\n",
    "final_processed_df.repartition(1).write.format(\"parquet\").mode(\"append\").save(f\"/opt/notebooks/nyc/processed_zone/trips/year={year}/month={month}/day={day}/first.parquet\")\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joining the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_processed_df.select(\"vendor_name\",\"vendor_id\",\"trip_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_processed_df.write.mode(\"append\").parquet(f\"/opt/notebooks/nyc/processed_zone/trips/year={year}/month={month}/day={day}/first.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
